%\documentclass[a4paper,14pt]{extarticle}
\documentclass[a4paper,11pt]{article}

%\usepackage[all-solutions-at-end]{optional}
\usepackage[check]{optional}

\usepackage{mathpazo}
\usepackage{preamble}

\opt{check}{
%\AtBeginEnvironment{exercise}{\clearpage}
\AtEndEnvironment{exercise}{\clearpage}
}

\opt{all-solutions-at-end}{
\Opensolutionfile{hint}
\Opensolutionfile{ans}
}

\title{Memoryless excursions\\
Notes on joint distributions }
\author{Nicky van Foreest}


\begin{document}
\maketitle
\tableofcontents



\section{A confusing problem}
\label{sec:confusing-problem}

Write $M=\max{X,Y}$ and $L=\min{X,Y}$ for given r.v.s $X$ and $Y$.
It is simple to see that when $X$ and $Y$  have the same distribution, 
\begin{equation}
\E L + \E M = \E{L+M}=\E{X+Y} = 2\E X.
\end{equation}
From this, we have that 
\begin{equation}
  \label{eq:6}
\E M = 2\E X - \E L.
\end{equation}
Next, when $X$ and $Y$ are memoryless and iid, it is  tempting to think that
\begin{equation}
  \label{eq:7}
\E M = \E L  + \E X,
\end{equation}
because of  the following interpretation.
There are two machines, each working on a job in parallel.
Let $X$ and $Y$ be the production times at either machine.
Then, we first wait until the first job finishes; this time is evidently $L=\min{X, Y}$.
Then, due to memorylessness, the service time of the remaining job `restarts'; this takes an expected time $\E X$ to complete.

Using~\cref{eq:6} and \cref{eq:7}  we can solve for $\E M$ and $\E L$.
Adding the two equations and noting that $\E L$ cancels we get $2\E M = 3 \E X $, hence:
\begin{align}
\E M &= \frac 3 2 \E X,\label{eq:35}\\
\E L &= \E M - \E X = \frac 1 2 \E X.\label{eq:36}
\end{align}
You might want to compare this to BH.7.2.2\footnote{BH: Blitzstein and Hwang}, where they analyze the case with uniform $X, Y$.

However, as we will see in the first set of exercises below, this is \emph{ not true for discrete} memoryless r.v.s, i.e., when $X, Y \sim \Geo{p}$, while it is true when $X, Y \sim \Exp{\lambda}$, cf., BH.5.6.5.
In the exercises below, I try to help you find out why this is so, and how to adapt~\cref{eq:6} and/or \cref{eq:7} so that they also hold when $X,Y\sim\Geo{p}$.

Actually, these exercises serve some much more general goals. They are meant to
\begin{enumerate}
\item become familiar with all concepts  BH 7.1.
\item  provide a recap of concepts and results of  BH chapters 1 to 6.
\item illustrate the change of variables formulas of BH 8.
\item  motivate to read (and memorize) the material of BH A1, A.2.5, A.6, A.7, A.8, A.9 and A.10.
\end{enumerate}
Once you solved all these exercises below, the exercises of BH will be much simpler.
All exercises below have solutions which focus on the computational steps. I will provide motivation and  explanations in the online lectures.

Below I use the notation $\partial_x = \partial / \partial x$, and $G_{X}(x)=1-F_{X}(x)$ for the survivor function of the r.v. $X$.

To solve the problems, I often  use the following step-wise approach:
\begin{enumerate}
\item To compute probabilities of functions of r.v.s: I use the fundamental bridge, specifically, for discrete r.v.s
  \begin{equation}
    \label{eq:60}
\P{g(X,Y}\in A\} = \sum_{i}\sum_j \1{(i, j)\in A} \P{X=i, Y=j}, 
  \end{equation}
and for continuous r.v.s, 
  \begin{equation}
    \label{eq:55}
    \begin{split}
\P{g(X, Y) \in A} 
&= \E{\1{g(X,Y)\in A}} \\
&= \iint \1{g(x,y) \in A} f_{X,Y}(x,y) \d x \d y \\
&= \iint_{g^{-1}(A)} f_{X,Y}(x, y) \d x \d y,\\
&= \iint_{\{(x,y): g(x,y) \in A\}} f_{X,Y}(x,y) \d x \d y.
    \end{split}
  \end{equation}
Conceptually this approach is easy, but simplifying  the summation or integration is not always simple.
However, the advantage is that I don't need special tricks or knowledge to solve the problem; it's just (quite a bit of) straightforward work.
With practice, this approach always works.
\item The joint CDF $F_{X,Y}$ follows from step 1, because $F_{X,Y}(x,y) = \P{X\leq x, Y\leq y} = \E{\1{X\leq x, Y\leq y}}$.
  In other words, I apply 2D LOTUS to the function $g(x,y) = \1{X\leq x, Y\leq y}$.
\item To get the joint density $f_{X,Y}$, I often first compute the joint distribution $F_{X,Y}$ and then take partial derivatives because $f_{X,Y} = \partial_{x}\partial_y F_{X,Y}$.
  This approach is `trick-free', but requires at times the hard work of solving the integrals to find $F_{X,Y}$.
\end{enumerate}

Solving the exercises of~\cref{sec:exerc-expon-distr} requires quite a lot of technical stuff with integrals.
With the tools of BH ch 8 this can be done (considerably) easier.
So in~\cref{sec:work-with-dens} we redo the work of~\cref{sec:exerc-expon-distr}, but now with change of variables techniques and densities.
Read it once you study BH ch 8.




In the exercises below we first try to see which of~\cref{eq:6} and \cref{eq:7} is not true for memoryless discrete r.v.s, then we consider memoryless continuous r.v.s. Once we have dealt with these two cases, we can  resolve the confusing problem.


\section{Discrete memoryless r.v.s}
\label{sec:set-1}


Let $X,Y$ be $\iid \sim \Geo{p}$. Write $q = 1-p$.

\begin{exercise}
As a recap of earlier chapters of BH, show that
\begin{align}
  \label{eq:9}
\P{X>j} &= q^{j+1}, &\P{X\geq j} &=q^j,& \E{X}&=\frac q{p}.
 \end{align}
What is the domain of $X$?
\begin{solution}
Of course $X \in \{0, 1, 2, \ldots\}$.

Here are a couple of tricks with recursions; this is how I rederive the properties of a geometric r.v.
\begin{align}
  \label{eq:8}
\P{X>0} &= \P{\textrm{failure}} = q\\
\P{X>j} &= q \P{X>j-1} \implies \P{X>j}=q^{j}\P{X>0} = q^{j+1}.\\
\P{X=j} &= \P{X>j-1} - \P{X>j} = q^{j}-q^{j+1} = (1-q)q^{j} = p q^{j}.\\
\E X &= p\cdot 0 + q(1+\E X) \implies \E X = q/(1-q) = q/p.
\end{align}

Now the same answers, but with the regular method. 
  \begin{align}
    \label{eq:5}
\P{X>j}    
&= \sum_{i=j+1}^{\infty} \P{X=i} \\
&= p\sum_{i=j+1}^{\infty} q^{i} \\
&= p\sum_{i=0}^{\infty} q^{j+1+i} \\
&= pq^{j+1}\sum_{i=0}^{\infty} q^{i} \\
&= pq^{j+1}\frac 1 {1-q} = p q^{j+1} \frac 1 p = q^{j+1}.
  \end{align}
From this,
  \begin{align}
    \label{eq:56}
  \P{X\geq j}  &= \P{X>j-1} = q^{j}, \\
  \end{align}

The use of indicator variables is particularly important:
  \begin{align}
    \label{eq:58}
\E X 
&= \sum_{i=0}^{\infty} i\P{X=i} \\
&=  \sum_{i=0}^{\infty} \sum_{j=0}^{\infty} \1{j<i} \P{X=i} \\
&=  \sum_{j=0}^{\infty} \sum_{i=0}^{\infty} \1{j<i} \P{X=i} \\
&=  \sum_{j=0}^{\infty} \sum_{i=j+1}^{\infty}  \P{X=i} \\
&=  \sum_{j=0}^{\infty}  \P{X>j} \\
&=  \sum_{j=0}^{\infty}  q^{j+1}\\
&=  q \sum_{j=0}^{\infty}  q^{j}\\
&=  q/(1-q) = q/p.
  \end{align}

\end{solution}
\end{exercise}

\begin{exercise}
Check that $X$, and hence $Y$, is memoryless.
\begin{hint}
What is the definition of memorylessness for discrete r.v.s?  
\end{hint}
\begin{solution}
  \begin{align}
    \label{eq:13}
\P{X\geq n+m\given X\geq m} 
&= \frac{ \P{X\geq n+m,  X\geq m}}{\P{X\geq m}} \\
&= \frac{ \P{X\geq n+m}}{\P{X\geq m}} \\
&= \frac{ q^{n+m}}{q^m} \\
&= q^n= \P{X\geq n}.
  \end{align}
\end{solution}
\end{exercise}

\begin{exercise}\label{ex:12}
Show that $\P{L\geq i}=q^{2i}$ and conclude that $L\sim\Geo{1-q^{2}}$. Just to keep you sharp: what is the domain of $L$?
\begin{solution}
  \begin{align}
    \label{eq:12}
\P{L\geq k} 
&= \sum_{ij} \1{\min{i,j}\geq k} \P{X=i, Y=j}\\
&= \sum_{i\geq k} \sum_{j\geq k} \P{X=i}\P{Y=j}\\
&=  \P{X\geq k}\P{Y\geq k} = q^{k} q^{k} = q^{2k}.
  \end{align}
$\P{L>i}$ has the same form as $\P{X>i}$, but now with $q^{2i}$ rather than $q^{i}$.
\end{solution}
\end{exercise}

As henceforth all r.v.s we consider are non-negative, I'll drop the question to specify the domain of the r.v.s. However, in general you should not forget to specify this. 



\begin{exercise}\label{ex:3}
Show that  $\E L=q^{2}/(1-q^{2})$.
\begin{hint}
$X\sim \Geo{1-q} \implies \E X = q/(1-q)$. Now use that $L\sim\Geo{1-q^{2}}$. 
\end{hint}
\begin{solution}
  Immediate from the hint and~\cref{ex:12}. 
\end{solution}
\end{exercise}

\begin{exercise}
Show that 
\begin{equation}
  \label{eq:39}
\E L + \E X = \frac q{1-q}\frac{1+2q}{1+q}.
\end{equation}
\begin{hint}
Use that $1-q^2=(1-q)(1+q)$.
\end{hint}
\begin{solution}
  \begin{align}
    \label{eq:2}
\E L     + \E X 
&= \frac{q^{2}}{1-q^2} + \frac{q}{1-q} \\
&= \frac q{1-q}\left(\frac{q}{1+q} + 1\right)\\
&= \frac q{1-q} \frac{1+2q}{1+q}
  \end{align}
\end{solution}
\end{exercise}

\begin{exercise}
Supposing that~\cref{eq:6} is true, show that
\begin{equation}
  \label{eq:10}
\E M = 2\E X  -\E L = \frac q{1-q} \frac{2+q}{1+q}.
\end{equation}
Show also that this is not the same as~\cref{eq:39} (unless $q=0$).
Conclude that this violates the simultaneous truth of~\cref{eq:6} and \cref{eq:7}.
\begin{solution}
  \begin{align}
    \label{eq:11}
\E M 
&= 2\E X - \E L \\
&= 2\frac q{1-q}  - \frac{q^2}{1-q^2}\\
&= \frac q{1-q}\left(2  - \frac{q}{1+q}\right)\\
&= \frac q{1-q}\left(\frac{2+2q}{1+q}  - \frac{q}{1+q}\right)\\
&= \frac q{1-q}\frac{2+q}{1+q}.
  \end{align}
\end{solution}
\end{exercise}


So, something is wrong.
I believe that~\cref{eq:6} is correct, but then~\cref{eq:7} must be wrong.
But why?
And, perhaps I am mistaken, and \cref{eq:7} is correct, and the other is not.

In fact, to convince myself that~\cref{eq:7} is indeed wrong, I pursued three ideas to check~\cref{eq:10}, hence~\cref{eq:6}

\begin{exercise}\label{ex:1}
Idea 1: Show that for the PMF of $M$: 
\begin{equation}
  \label{eq:34}
p_M(k) = \P{M=k} = 2 pq^{k}(1-q^{k}) + p^2q^{2k}.
\end{equation}
Then use this to show~\cref{eq:10}.
\begin{solution}
  \begin{align}
    \label{eq:14}
\P{M=k}     
&= \P{\max{X, Y} = k} \\
&=p^{2}\sum_{ij}\1{\max{i,j} =k} q^i q^j\\
&=2 p^{2}\sum_{ij}\1{i=k}\1{j < k} q^i q^j + p^{2}\sum_{ij}\1{i=j=k} q^{i} q^j \\
&=2 p^{2}q^{k}\sum_{j < k} q^j + p^{2}q^{2k}\\
&=2 p^{2}q^{k}\frac{1-q^{k}}{1-q} +  p^{2}q^{2k}\\
&=2 pq^{k}(1-q^{k}) + p^{2}q^{2k}\\
&=2 pq^{k} +(p^{2}-2p) q^{2k} \label{eq:18}\\
&=2 \P{X=k} - \P{L=k},
  \end{align}
where I use that $p^{2}-2p = p(p-2) = (1-q)(1-q-2)=-(1-q)(1+q)=-(1-q^{2})$.
  \begin{align}
    \label{eq:15}
\E M 
&= \sum_k k \P{M=k}     \\
&=  \sum_{k}k (2\P{X=k} - \P{L=k}) \\
&= 2 \E X  - \E L.
  \end{align}
\end{solution}
\end{exercise}

\begin{exercise}
Idea 2: Use  $\P{M\leq k} = (\P{X\leq k})^{2}$  to check~\cref{eq:34}.
\begin{hint}
$\P{M=k} = \P{M\leq k} - \P{M\leq k-1}$.
\end{hint}
\begin{solution}
  \begin{align}
    \label{eq:16}
\P{M\leq k}   
&= \P{X\leq k, Y\leq k} \\
&= \P{X\leq k}\P{Y\leq k} \\
&= (1-\P{X> k})(1-\P{Y> k}) \\
&= (1-q^{k+1})^{2}.\\
\P{M=k} &= \P{M\leq k} - \P{M\leq k-1}\\
&= 1-2 q^{k+1} + q^{2k+2} - (1 - 2q^{k} + q^{2k})\\
&= 2 q^{k}(1-q) + q^{2k}(q^{2}-1) \\
&= 2 \P{X=k}  - q^{2k}(1-q^{2}) \\
&= 2 \P{X=k}  - \P{L=k}.
  \end{align}
\end{solution}
\end{exercise}


\begin{exercise}\label{ex:2}
Idea 3a: show that  the  joint PMF
\begin{equation}
p_{L,M}(i,j) = \P{L=i, M=j} = 2p^{2}q^{i+j}\1{j>i}+ p^{2}q^{2i}\1{i=j}.
\end{equation}
\begin{solution}
  \begin{align}
    \label{eq:17}
\P{L=i, M=j}    
&= 2\P{X=i, Y=j}\1{j>i} + \P{X=Y=i}\1{i=j} \\
&= 2p^{2}q^{i+j}\1{j>i}+ p^{2}q^{2i}\1{i=j}.
  \end{align}
\end{solution}
\end{exercise}

\begin{exercise}\label{ex:8}
Idea 3b: Use~\cref{ex:2} to compute the marginal PMFs of $M$ and of $L$. 
\begin{hint}
For the first, marginalize out $L$, for the second, marginalize out $M$.
\end{hint}
\begin{solution}
  \begin{align}
\P{M=j}    
&= \sum_{i} \P{L=i, M=j} \\
 &= \sum_{i} (2p^{2}q^{i+j}\1{j>i}+ p^{2}q^{2i}\1{i=j}) \\
 &= 2p^2q^j\sum_{i=0}^{j-1} q^{i}+ p^{2}q^{2j} \label{eq:38}\\
 &= 2pq^j (1-q^{j})+ p^{2}q^{2j} \\
 &= 2pq^j + (p^{2}-2p) q^{2j},
  \end{align}
which is equal to \cref{eq:18}.
  \begin{align}
\P{L=i}    
&= \sum_{j} \P{L=i, M=j} \\
 &= \sum_{j} (2p^{2}q^{i+j}\1{j>i}+ p^{2}q^{2i}\1{i=j}) \\
 &= 2p^2q^i\sum_{j=i+1}^{\infty} q^{j}+ p^{2}q^{2i} \\
 &= 2p^{2}q^{2i+1} \sum_{j=0}^{\infty}q^{j}+ p^{2}q^{2i} \\
 &= 2pq^{2i+1} + p^{2} q^{2i}\\
 &= pq^{2i}(2q+p)\\
 &= (1-q)q^{2i}(q + 1), \quad p+q=1,\\
 &= (1-q^{2})q^{2i},
  \end{align}
and this we found earlier.
\end{solution}
\end{exercise}


Finally, BH Ex.5.6.5 use that the r.v.
$M-L$ is independent of $L$ when $X, Y\sim\Exp{\lambda}$ and independent.
But this leads to our confusing problem, so perhaps my intuition about memorylessness is not ok.
To check this, I solved  the following exercise.  

\begin{exercise}\label{ex:10}
Show that the events $\{L=i\}$ and $\{M>L\}$ are independent when $X,Y \sim \Geo{p}$.
\begin{hint}
Show that $\P{L=i \given M>L} = \P{L=i}$; recall, to show independence we need the probability (measure) $\P{\cdot}$.

Use the expression $\P{g(X,Y)\in A} = \mathop{\sum\sum}_{g(i, j) \in A} p_{X,Y}(i,j)$
\end{hint}
\begin{solution}
\begin{align}
    \label{eq:1}
\P{L=i\given M > L} 
=\frac{\P{L=i, M > L}}{\P{M>L}}.
\end{align}

\begin{align}
\P{L=i,  M > L} 
&= 2\P{X=i,Y > X} \\
&= 2\P{X=i}\P{Y > i} \\
&= 2pq^i\cdot q^{i+1}\\
&= 2p q^{2i+1}.
\end{align}

I use three (related) ways to compute $\P{M>L}$. In the first I convert the event $\{M>L\}$ directly into a statement about $X$ and $Y$.
\begin{align}
  \label{eq:3}
\P{M>L} 
&= \P{X>Y} + \P{Y > X} \\
&= 2\P{X>Y} \\
&= 2\sum_{j} \P{X>j,Y=j} \\
&= 2\sum_{j}  \P{X>j}\P{Y=j} \\
&= 2p\sum_{j} q^{j+1} q^j\\
&= 2p\sum_{j} q^{2j+1}\\
&= 2pq\sum_{j} q^{2j}\\
&= 2pq/(1-q^2).
\end{align}
In the second,  I rewrite the event $\{M>L\}$ as a function of $X$ and $Y$, and then use  the formula of the hint:
\begin{align}
  \label{eq:4}
\P{M>L}  &= \P{\max{X, Y} > \min{X,Y}} \\
&=p^2\sum_{ij}\1{\max{i, j} > \min{i, j}} p_{XY}(i,j) \\
&=p^2\sum_{ij}\1{\max{i, j} > \min{i, j}} p_{X}(i)p_{Y}(j) \\
&=p^2\sum_{ij}\1{\max{i, j} > \min{i, j}} q^i q^j \\
&=2p^2\sum_{i}\sum_{j>i} q^i q^j \\
&=2p^2\sum_{i}q^i \sum_{j>i} q^j \\
&=2p^2\sum_{i}q^i \sum_{j=0}^\infty q^{i+1+j} \\
&=2p^2\sum_{i}q^{2i+1} \sum_{j=0}^\infty q^{j} \\
&=2p\sum_{i}q^{2i+1} \\
&=2pq\sum_{i}q^{2i} \\
&=2pq/(1-q^2).
\end{align}
In the third, I use the probability $\P{L=i, M>L}$ and take the sum over $i$:
\begin{align}
  \label{eq:48}
\P{M>L} = \sum_{i} \P{L=i, M>L}  = \sum_{i} 2pq^{2i+1} = 2pq\sum_{i} q^{2i} = \frac{2pq}{1-q^{2}}.
\end{align}

Hence, 
\begin{align}
\P{L=i\given M > L} 
&=\frac{\P{L=i, M > L}}{\P{M>L}} \\
&=\frac{2p q^{2i+1}}{2pq/(1-q^2)} \\
&=q^{2i}(1-q^2) \\
&= \P{L=i},
\end{align}
where the last equation follows since $L\sim \Geo{1-q^2}$.

\end{solution}
\end{exercise}


What have we achieved up to now? With respect to our problem, we are sure that~\eqref{eq:6} is correct when $X, Y \sim \Geo{p}$ and~\cref{eq:7} is not.
From the technical point of view, we practiced with joint, marginal, and conditional PMFs, and we have seen how to use indicator functions and the fundamental bridge.

So, this made me wonder why is~\eqref{eq:7} incorrect when $X, Y\sim\Geo{p}$, but it is correct when $X,Y\sim\Exp{\lambda}$.
To understand this discrepancy better, I decided to analyze the latter case in as much detail as I could think of, hoping that this would  provide me with a lead.
The work will be pretty technical at times, but there is no way around when dealing with joint densities, etc.



\section{Continuous memoryless r.v.s}
\label{sec:exerc-expon-distr}

In this section we analyze the correctness of~\cref{eq:6} and \cref{eq:7} for continuous memoryless r.v.s, i.e., exponentially distributed r.v.s.
This will offer a good opportunity to practice with several concepts: joint, marginal and conditional distributions of continuous r.v.s,  limits, and moment-generating functions.
We don't only solve the problem of~\cref{sec:confusing-problem}, in passing we will obtain a lot of necessary and useful practice.

First we need to recall some basic facts about the exponential distribution.

\begin{exercise}
Show that $X$, and hence $Y$, is memoryless.
\begin{solution}
  See BH.
\end{solution}
\end{exercise}


\begin{exercise}\label{ex:4}
Show that $\E X = 1/\lambda$.
\begin{hint}
  Use partial integration.
\end{hint}
\begin{solution}
It is essential that  you know both methods to solve this integral.
  \begin{align}
\E X 
&= \int_{0}^{\infty}  x f_{X}(x) \d x  \\
&= \int_{0}^{\infty}  x \lambda e^{-\lambda x} \d x  \\
&= - xe^{-\lambda x} \biggr|_{0}^{\infty} + \int_{0}^{\infty}  e^{-\lambda x} \d x  \\
&=  - \frac 1 \lambda e^{-\lambda x} \biggr|_{0}^{\infty} = \frac 1 \lambda.
  \end{align}
Substitution is also a very important technique to solve such integrals. Here we go again:
  \begin{align}
\E X 
&= \int_{0}^{\infty}  x f_{X}(x) \d x  \\
&= \int_{0}^{\infty}  x \lambda e^{-\lambda x} \d x  \\
&= \frac 1 \lambda \int_{0}^{\infty}  u  e^{- u} \d u, 
  \end{align}
  by the substitution $u=\lambda x \implies \d u = \d (\lambda x) \implies \d u = \lambda \d x \implies \d x = \d u / \lambda$.
  With partial integration (do it!), the integral evaluates to $1$.
\end{solution}
\end{exercise}


Now we can shift our attention to the r.v.s $L$ and $M$.


\begin{exercise}
  Show that $F_L(x) = 1-e^{-2\lambda x}$, hence  $L\sim \Exp{2\lambda}$ and therefore $\E L = 1/(2\lambda)$.
  Realize that with this we have an independent check of~\cref{eq:36}.
\begin{solution}
Using independence and the specific property of the r.v. $L$ that $\{L>x\} \iff \{X>x, Y>x\}$:
 \begin{align}
    \label{eq:22}
G_L(x) = \P{L>x}  &= \P{X>x, Y> x} = G_X(x)^{2} = e^{-2\lambda x}.
  \end{align}
The result follows since  $F_{L}(x) = 1-G_{L}(x)$.
\end{solution}
\end{exercise}

\begin{exercise}\label{ex:6}
Compute $F_M(v) = \P{M\leq v}$. With this, show that  $f_{M}(v)=2(1-e^{-\lambda v}) \lambda e^{-\lambda v}$,  and then use this to compute $\E M$ to show that~\cref{eq:35} holds.
\begin{solution}
Here are two related ways. This is one:
\begin{align}
\P{M\leq v}   &= \P{X \leq v, Y \leq v} = (F_{X}(v))^{2},
\end{align}
since $X, Y$ iid.  

The second is based on the fundamental bridge:
\begin{align}
\P{M\leq v}   
&= \E{\1{M\leq v}} \\ 
&=\int_0^{\infty} \int_0^{\infty} \1{x\leq v, y\leq v} f_{XY}(x,y) \d x \d y \\
&= \int_0^{\infty} \int_0^{\infty} \1{x\leq v, y\leq v} f_{X}(x) f_{Y}(y) \d x \d y \\
&= \int_0^{v} f_{X}(x) \d x \int_0^{v}  f_{Y}(y)  \d y \\
&= F_{X}(v) F_Y(v) = (F_X(v))^2.
\end{align}
We get the same result.

Now  the density:
\begin{align}
  \label{eq:26}
  f_M(v) = F_M'(v) = 2 F_X(v) f_X(v) = 2(1-e^{-\lambda v}) \lambda e^{- \lambda v}.
\end{align}
With this, 
\begin{align}
  \label{eq:27}
\E M 
&= \int_{0}^{\infty} v f_M(v) \d v = \\
&= 2 \lambda \int_{0}^{\infty} v (1-e^{-\lambda v}) e^{-\lambda v} \d v = \\
&= 2 \lambda \int_{0}^{\infty} v e^{-\lambda v} \d v -  2 \lambda \int_{0}^{\infty} v e^{-2 \lambda v} \d v \\
&= 2 \E X - \E L, 
\end{align}
where the last equality follows from the previous exercises. 
\end{solution}
\end{exercise}

We can also compute $f_{M}(y)$ (and $f_{L}(x)$ by marginalization of the joint density $f_{L,M}(x,y)$ .
However, to get $f_{L,M}$, we first need $F_{L,M}$.
\begin{exercise}
Use the fundamental bridge~\cref{eq:55} to show that
\begin{equation*}
  F_{L,M}(u,v) = \P{L\leq u, M\leq v} = 2\int_0^u (F_Y(v)- F_Y(x)) f_X(x) \d x, 
\end{equation*}
with $u\leq v$ (of course). Then take partial derivatives to show that
\begin{align}
  \label{eq:52}
f_{L,M}(u,v) = 2f_X(u) f_Y(v)\1{u\leq v}.
\end{align}

\begin{solution}
First the joint distribution. With $u\leq v$,
  \begin{align}
    \label{eq:51}
F_{L,M}(u,v) &= \P{L\leq u, M \leq v} \\
&= 2\iint \1{x \leq u, y\leq v, x\leq y} f_{X,Y}(x,y)\d x \d y \\
&= 2\int_0^u \int_x^v f_Y(y) \d y f_X(x) \d x & \text{independence} \\
&= 2\int_0^u (F_Y(v)- F_Y(x)) f_X(x) \d x. 
  \end{align}
Taking partial derivatives,
\begin{align}
f_{L,M}(u,v) 
&=\partial_v\partial_{u}F_{L,M}(u,v) \\  
&=2 \partial_v\partial_{u} \int_0^u (F_Y(v)- F_Y(x)) f_X(x) \d x  \\
&=2 \partial_v \left\{(F_Y(v)- F_Y(u)) f_X(u) \right \}  \\
&=2 f_X(u)\partial_v F_Y(v)  \\
&=2 f_X(u)f_Y(v).
\end{align}
\end{solution}
\end{exercise}

\begin{exercise}
In~\cref{eq:52} marginalize out $L$ to find $f_M$, and marginalize out $M$ to find $f_L$.
\begin{solution}
\begin{align}
  \label{eq:54}
  f_M(v) &=  \int_0^{\infty} f_{L,M}(u,v) \d u \\
&= 2 \int_0^{v} f_{X}(u) f_Y(v) \d u \\
&= 2 f_Y(v) \int_0^{v} f_{X}(u) \d u  \\
&= 2f_Y(v) F_X(v), \\
f_L(u) &=  \int_0^{\infty} f_{L,M}(u,v) \d v \\
&= 2 f_{X}(u) \int_u^{\infty} f_Y(v) \d v \\
&= 2 f_{X}(u) G_{Y}(u).
\end{align}
\end{solution}
\end{exercise}

We can conclude that~\cref{eq:35} and \cref{eq:36} are correct for $X,Y\sim\Exp{\lambda}$.
But, I have yet another way to check this, namely, from~\cref{eq:7} I see that $\E{M-L} = \E X$.

Let's try to verify that also.
For this, we can first compute the conditional density $f_{M-L| L}(y| x)$; once you did the next exercise you will directly see how to compute $\E{M-L}$.

\begin{exercise}\label{ex:5}
Show that 
\begin{align}  \label{eq:23}
F_{L,M-L}(x,y) &= (1-e^{-\lambda y})(1- e^{-2\lambda x}) = F_Y(y) F_L(x).
\end{align}
% hence, 
% \begin{align}
%   \label{eq:25}
% f_{L, M-L}(x,y )  &= f_L(x)f_Y(y)
% \end{align}
Conclude that $M-L$ and $L$ are independent, and $M-L\sim Y$, hence $\E{M-L} = \E Y = \E X$, as $X$ and $Y$ are iid.
\begin{solution}
I'll use the standard method, i.e., the fundamental bridge.
In itself the idea is not difficult, but the technical details require attention, in particular the limits in the integrations.
\begin{align} 
\P{L\leq x, M-L\leq y} 
&= 2\P{X\leq x, Y-X \leq y, X \leq Y} \\
&= 2\int_0^{\infty} \int_0^{\infty} \1{u\leq x, v-u\leq y, u \leq v} f_{X,Y}(u, v)  \d u \d v\\
&= 2\int_0^{\infty} \int_0^{\infty} \1{u\leq x, v-u\leq y, u\leq v} \lambda^2e^{-\lambda u}e^{-\lambda v}\d u \d v\\
&= 2\int_0^{x} \int_0^{\infty} \1{u\leq v\leq u+y} \lambda^2e^{-\lambda u}e^{-\lambda v}\d v \d u\\
&= 2\int_0^{x} \lambda e^{-\lambda u} \int_u   ^{u + y}  \lambda e^{-\lambda v}\d v \d u\\
&= 2\int_0^{x} \lambda e^{-\lambda u} (-e^{-\lambda v}) \biggr|_u^{u+y}\d u \\
&= 2\int_0^{x} \lambda e^{-\lambda u} (e^{-\lambda u} - e^{-\lambda (u+y)})\d u \\
&= 2\lambda\int_0^{x}  e^{-2 \lambda u}\d u  - 2\lambda \int_{0}^{x} e^{-\lambda (2u+y)}\d u \\
&= 2\lambda\int_0^{x}  e^{-2 \lambda u}\d u  - 2\lambda e^{-\lambda y }\int_{0}^{x} e^{-2 \lambda u}\d u \\
&= (1-e^{-\lambda y}) 2 \lambda \int_0^{x} e^{-2 \lambda u} \d u\\ 
&= (1-e^{-\lambda y}) ( -e^{-2\lambda u}) \biggr|_0^{x} \\ 
&= (1-e^{-\lambda y})(1- e^{-2\lambda x}).
\end{align}
\end{solution}
\end{exercise}


We did a number of checks for the case $X, Y, \iid, \sim\Exp{\lambda}$.
But I have still another idea to see whether the results we have obtained so far are consistent.
For this I use BH Ex 5.43 in which it is shown that the geometric distribution is the discrete analog of the exponential distribution.
Now I want to see that  by taking proper limits, I can  obtain the same results.

More specically, I want to see intuitively\footnote{A formal proof is too hard for this course.} how $X\sim \Geo{\lambda/n}$ approaches  $Y\sim\Exp{\lambda}$ as $n\to\infty$.

\begin{exercise}\label{ex:7} 
Divide the interval $[0, \infty)$ into many small intervals of length $1/n$.
Let  $X\sim \Geo{\lambda/n}$ for some $\lambda>0$ and $n\gg 0$. Then take  some $x\geq 0$ and  let $i$ be such that $x\in[i/n, (i+1)/n)$.
Show that 
\begin{align}\label{eq:53}
\P{X/n \approx x} \approx \frac{\lambda} n \left(1-\frac \lambda n\right)^{xn}.
\end{align}
\begin{solution}
First, 
\begin{align}
\P{X/n \approx x} &= \P{X/n \in [i/n, (i+1)/n]} = \P{X \in [i, i+1]} = p q^{i} \\
&\approx p q^{n x} = \frac{\lambda} n \left(1-\frac \lambda n\right)^{xn},
\end{align}
since $p=\lambda/n$, $q=1-p=1-\lambda/n$, and $i=nx$.
\end{solution}
\end{exercise}
From BH.A.2.5, $\lim_{n\to\infty}(1-\lambda/n)^{n} = e^{-\lambda}$.\footnote{This is a standard limit but not entirely trivial to prove. If you like mathematics, check the neat proof in Rudin's Principles of mathematical analysis.}
Next, by introducing the highly intuitive definition $\d x = \lim_{n\to \infty} 1/n$,\footnote{In your math classes you learned that $\lim_{n\to \infty} 1/n = 0$. Doesn't this definition therefore imply that $\d x = 0$? Well, no, because $\d x$ is not a real number but an infinitesimal. Infinitesimals allow us to consider a quantity that is so small that it cannot be distinguished from 0 within the real numbers.}  it follows from~\cref{eq:53} that
\begin{align}
\P{X/n \approx x} \to \lambda e^{-\lambda x} \d x = f_{X}(x) \d x,\quad\text{ as } n \to \infty.
\end{align}
% In fact, working with infinitesimals such as $\d x$ is one of my favorite methods, for instance to derive densities.
% The intuition is that $\d x$ is some really small number but not zero.
% In a tiny bit more formal words, $0<\d x < 1/n$ for any $n$, but $\d x \neq 0$.
% For details, check out the web; search on `nonstandard calculus'.
% However, while the method is very intuitive, it's easy to mess up. (Power tools are nice, but often only when you know how to handle them and practice a lot.) 
% If you don't want to learn this, no problem, it will not be part of the course proper.

If you don't like this trick with $\d x$, is also possible obtain this limit with moment-generating functions.

\begin{exercise}
Derive the moment-generating function $M_{X/n}(s)$ of $X/n$ when $X\sim \Geo{p}$.
Then, let $p = \lambda/n$, and show that $\lim_{n\to\infty}M_{X/n}(s) = M_{Y}(s)$, where $Y \sim \Exp{\lambda}$.
\begin{hint}
  If you recall the Poisson distribution, you know that $e^{\lambda} = \sum_{i=0}^{\infty}\lambda^{i}/i!$.
  In fact, this is precisely Taylor's expansion of $e^{\lambda}$.
\end{hint}
\begin{solution}
  \begin{align}
    \label{eq:20}
M_{X/n}(s) 
&= \E{e^{s X/n}} = \sum_{i} e^{s i/n} p q^{i} = p\sum_{i} (qe^{s/n})^{i} = \frac{p}{1-qe^{s/n}}.
  \end{align}
With $p=\lambda/n$ this becomes
  \begin{align}
M_{X/n}(s) 
&= \frac{\lambda/n}{1-(1-\lambda/n) (1+s/n + 1/n^{2}\times(\cdots))} \\
&= \frac{\lambda/n}{\lambda/n - s/n + 1/n^{2}\times (\cdots)} \\
&= \frac{\lambda}{\lambda - s + 1/n\times(\cdots)} \\
&\to \frac{\lambda}{\lambda - s}, \quad\text{as }  n\to \infty,
  \end{align}
  where we write $1/n^{2}\times(\cdots)$ for all terms that will disappear when we take the limit $n\to \infty$.
  This is just handy notation to hide details in which we are not interested.
\end{solution}
\end{exercise}

\begin{exercise}\label{ex:11}
Check that $\lim_{n\to\infty}\E{L/n} = 1/(2\lambda)$ when $X\sim \Geo{\lambda/n}$. Conclude that  we retrieve~\cref{eq:36}.
\begin{hint}
  Use~\cref{ex:3}.
\end{hint}
\begin{solution}
  \begin{align}
    \label{eq:21}
\E{L/n}  &= \frac 1 n \E L = \frac 1 n \frac{q^2}{1-q^{2}} \\
 &= \frac 1 n \frac{(1-\lambda/n)^2}{1-(1-\lambda/n)^{2}} \\
 &= \frac 1 n \frac{1-2 \lambda/n + (\lambda/n)^2}{2 \lambda /n + (\lambda/n)^{2}} \\
 &= \frac{1-2 \lambda/n + (\lambda/n)^2}{2 \lambda + \lambda^{2}/n} \\
&\to \frac 1{2\lambda}.
  \end{align}
\end{solution}
\end{exercise}


\begin{exercise}
Here is yet another check on the correctness of $f_M(x)$. 
Show that  the PMF $\P{M=k}$ of~\cref{eq:34} converges to $f_M(x)$ of~\cref{ex:6} when $n\to \infty$. Take $k$ suitable.
\begin{solution} Take $p=\lambda/n$,  $q=1-\lambda/n$, and $k \approx x n$, hence $k/n \approx x$. Then,
  \begin{align}
    \label{eq:31}
\P{M/n=k/n}  
&= 2 p q^{k/n}(1-q^{k/n}) + p^2q^{2k/n} \\
&= 2 \frac \lambda n \left(1-\frac \lambda n\right)^{k/n}\left(1-\left(1-\frac \lambda n\right)^{k/n}\right) + \frac{\lambda^2}{n^2}\left(1-\frac \lambda n \right)^{2k/n} \\
&\to 2 \lambda \d x e^{-\lambda x} \left(1 - e^{-\lambda x}\right) + \lambda^{2} \d x^{2}e^{-2\lambda}.\label{eq:37}
  \end{align}
Now observe that the second term, proportional to $\d x^{2}$ can be neglected. 
\end{solution}
\end{exercise}

All in all, we have checked and double checked all our expressions and limits for the geometric and exponential distribution.
We had success too: the solution of the last exercise provides the key to understand why~\eqref{eq:6} and \eqref{eq:7} are true for exponentially distributed r.v.s, but not for geometric random variables.
In fact, in the solutions we can see the term corresponding to $X=Y=i$ for $X,Y\sim \Geo{p}$ becomes negligibly small when $n\to 0$.
In other words, $\P{X=Y}>0$  when $X$ and $Y$ are discrete, but $\P{X=Y}=0$ when  $X$ and $Y$ are continuous.
So, to resolve our leading problem we should exploit that idea.

\section{The solution}
\label{sec:solution}


Let us now try to repair~\cref{eq:7}  for the case $X,Y\sim\Geo{p}$. We should be careful about the non-negligible case that $M=L$, so we move on, carefully, step by step. The following must be true:
\begin{align}
  \label{eq:40}
\E M &= \E L + \E{(M-L) \1{M>L}}  = \E L + 2\E{(Y-X) \1{Y>X}},
\end{align}
because either $M=L$ or $M>L$.
Recall from earlier work that the factor 2 in the second equality follows from the fact that $X,Y$ iid.

\begin{exercise}\label{ex:9}
Show that 
\begin{equation}
  \label{eq:41}
2\E{(Y-X) \1{Y>X}} = \frac{2q} {1-q^{2}}.
\end{equation}
Combine this with the expression for $\E L$ of~\cref{ex:3} to obtain \cref{eq:10} for $\E M = 2\E X - \E L$, thereby verifying the correctness of~\cref{eq:40}. 
\begin{solution}
  \begin{align}
    \label{eq:42}
\E{(Y-X) \1{Y>X}} 
&= p^2\sum_{ij} (j-i)\1{j>i} q^iq^j\\
&= p^2\sum_{i} q^i\sum_{j=i+1}^{\infty}(j-i)q^j\\
&= p^2\sum_{i} q^i q^{i}\sum_{k=1}^{\infty}kq^k, \quad k = j-i\\
&= p\sum_{i}  q^{2i}\E X \\
&= \frac{p}{1-q^{2}}\E X \\
&= \frac{p}{1-q^{2}}\frac q p \\
&= \frac{q}{1-q^{2}}.
  \end{align}
With this, 
\begin{align}
  \label{eq:43}
\E L + 2  \E{(Y-X) \1{Y>X}} = \frac{q^{2}}{1-q^2} +  \frac{2q}{1-q^{2}} = \frac q{1-q}\frac{q+2}{1+q},
\end{align}
where I use that $1-q^{2}=(1-q)(1+q)$.
\end{solution}
\end{exercise}

While~\cref{eq:40} is correct, I am still not happy with the second part of~\eqref{eq:40} (I find this hard/unintuitive to interpret).
Restarting again from scratch, here is another attempt to rewrite $\E M$: 
\begin{align}
  \label{eq:44}
\E M = \E L + \E{ Z \1{M>L}},
\end{align}
where I take $Z\sim \FS{p}$, i.e., $Z$ has the first success distribution with parameter $p$, in other words, $Z \sim X+1$ with $X\sim\Geo{p}$.
To see why this might be true, I reason like this.
After `seeing' $L$, we want to restart.
Let $Z$ be the time from the restart to $M$.
When $Z\sim \Geo{p}$, it might happen that $Z=0$ (with positive probability $p$).
But if $Z=0$, then $M=L$, and it that case, we should not restart.
Hence, if $Z\sim \Geo{p}$ we are `double counting' when $Z=0$.
By including the condition $M>L$ and by taking $Z\sim\FS{p}$ (so that $Z>0$) I can prevent this.

\begin{exercise}
Show that 
\begin{equation}
  \label{eq:45}
\E{Z\1{M>L}} = \frac{2q} {1-q^{2}},
\end{equation}
i.e., the same as~\cref{eq:41}, hence~\cref{eq:44} is correct.
\begin{hint}
Observe that $Z$   is independent from $X$ and $Y$, hence from $M$ and $L$
\end{hint}
\begin{solution} With the hint:
  \begin{align}
    \label{eq:46}
\E{Z\1{M>L}}  = \E Z \E{\1{M>L}} = \E Z \P{M>L} = \frac 1 p \frac{2pq}{1-q^{2}} = \frac{2q}{1-q^{2}},
  \end{align}
where, from the book, $\E Z = 1/p$, and $\P{M>L}$ follows from~\cref{ex:10}. 
\end{solution}
\end{exercise}


I am nearly happy, but I want to see that~\eqref{eq:44}, which is correct for discrete r.v.s, has also the correct limiting behavior, similar to~\cref{ex:11}. 
\begin{exercise}
Show that $\E{Z/n\1{M>L}} \to 1/\lambda$, which is indeed the expectation of an $\Exp{\lambda}$ r.v. And thus, when $X,Y\sim \Exp{\lambda}$, $\E M = \E L + \E X$.
\begin{solution}
By independence,
  \begin{align}
    \label{eq:47}
\E{Z/n\1{M>L}} = \E{Z/n} \P{M>L}. 
  \end{align}
Then,
\begin{align}
  \label{eq:49}
\P{M>L}   = \frac{2pq}{1-q^{2}} 
= \frac{2\lambda/n (1-\lambda/n)}{1-(1-\lambda/n)^{2}} 
= \frac{2\lambda/n (1-\lambda/n)}{2\lambda/n -\lambda^{2}/n^{2}}  
= \frac{2 (1-\lambda/n)}{2 -\lambda/n}  \to 1,
\end{align}
and
\begin{align}
  \label{eq:50}
\E{Z/n} = \frac 1 n \E Z = \frac 1 n \frac 1 p = \frac 1 {n \lambda/ n} = 1/\lambda.
\end{align}
\end{solution}

\end{exercise}

It took me a long time, and a lot of work, to understand how to resolve the confusing problem.
Perhaps how to tackle this problem is clear to any other person, but I had a nice time working on it, and I learned a lot.
In particular, I find~\cref{eq:44} a nice and revealing equation.



\section{Working with densities}
\label{sec:work-with-dens}
You should notice that working with densities is often easier than working with the fundamental bridge because you can work with equalities rather than inequalities.
However, don't forget to properly scale lengths and areas; use the Jacobian!


\begin{exercise}
Derive the density $f_L(u)$ from the joint density $f_{X,Y}$.
\begin{solution}
Hence, with $u=x$,
\begin{align}
\P{L \approx u} = 2 \P{X \approx x, Y > x}. 
\end{align}
Hence, as $u=x \implies \d u/\d x = 1$, 
\begin{align*}
  f_{L}(u) \d u = 2f_{X}(x) G_{Y}(x) \d x \implies   f_{L}(u)  = 2f_{X}(u) G_{Y}(u).
\end{align*}
\end{solution}
\end{exercise}

\begin{exercise}\label{ex:6}
Derive the density $f_M(v)$ from the joint density $f_{X,Y}$.
\begin{solution}
With $v=y$
\begin{align}
\P{M \approx v} = 2 \P{X \leq v, Y \approx v}. 
\end{align}
As $v=y \implies \d v/\d y = 1$, 
\begin{align}
f_M(v) \d v =  2F_{X}(y) f_{Y}(y)\d y \implies f_M(v) =  2F_{X}(v) f_{Y}(v).
\end{align}
\end{solution}
\end{exercise}

\begin{exercise}
Find a function $g$ that maps $X,Y$ to $L,M$. Then compute $f_{L,M}$ by using change of variables. 
\begin{solution}
We need $g$ first, and then we compute the Jacobian. 
\begin{align}
(u,v) = g(x, y) = (\min{x,y}, \max{x,y}).
\end{align}
Hence, $g^{-1}(u,v) = \{(u,v), (v, u)\}$; note that this occurs because $g$ is not one-to-one.
However, this is not a real problem because we can easily keep track of `each branch' of $g^{-1}$. 

Next, noting that $\partial_{x}\min{x,y} = \1{x\leq y}$, 
\begin{align}
\frac{\partial (u,v)}{\partial(x,y)} = 
  \begin{vmatrix}
    \1{x\leq y} & \1{y<x} \\
    \1{x > y} & \1{y \geq x} \\
  \end{vmatrix} = |\1{x\leq y} - \1{x>y}| = 1.
\end{align}

Now with the change of variable formula in which I drop the Jacobian right away as it is $1$ and substitute $u=x$ and $y=v$,
\begin{align}
  \label{eq:30}
f_{L,M}(u,v)\1{u\leq v} = 2f_{X,Y}(u,v)\1{u\leq v}.
\end{align}
\end{solution}
\end{exercise}



\begin{exercise}
Here is another opportunity to check our work, but now with changing variables, cf. Chapter 8.
Find $f_{L,M-L}(x,y)$ by means of changing of variables from $L,M$ to $L,M-L$.
\begin{solution}
  The function $h(x,y)= (x, y-x)\1{x\leq y}$ maps $(L,M)$ to $(L, M-L)$. Then, $h^{-1}(u,v) = (u, u+v)$. For the Jacobian
  \begin{align}
    \label{eq:57}
\frac{\partial (u,v)}{\partial(x,y)} = 
    \begin{vmatrix}
      \1{x\leq y} & 0 \\
-\1{x\leq y} & \1{x\leq y}
    \end{vmatrix} = \1{x\leq y}.
  \end{align}
And with this,
\begin{align}
  \label{eq:19}
f_{L, M-L}(u,v) 
&= f_{L, M}(u, u+v) \1{u\leq u+v}  \\
&= f_{L,M}(u, u+v) \\
&= 2 f_{X}(u) f_Y(u+v) \\
&= 2 \lambda^{2}e^{-2\lambda u} e^{-\lambda v} \\
&= f_{L}(u) f_Y(v).
\end{align}

\end{solution}
\end{exercise}


Now two exercises just to practice with the method.

\begin{exercise}
Compute $f_{L^2}(l)$, where $L=\min{X,Y}$ and $X, Y$ iid $\sim \Exp{\lambda}$. 
\begin{solution}
With $l = x^{2}, x\leq y$
\begin{align*}
f_{L^2}(l) \d l = 2f_{X}(x) G_{Y}(x) \d x.
\end{align*}
As $\d l/\d x = 2 x = 2 \sqrt l$, 
\begin{align*}
f_{L^2}(l)  = 2f_{X}(\sqrt{l}) G_{Y}(\sqrt l ) \d x / \d l = f_{X}(\sqrt{l}) G_{Y}(\sqrt l ) /\sqrt l.
\end{align*}
\end{solution}

\end{exercise}


\begin{exercise}
We have three r.v.s, $X, Y, Z$, iid. Let $L=\min{X, Y, Z}$ and $M=\max{X, Y, Z}$. Find $f_{L,M}(u,v)$.
\begin{solution}
There are $3!$ ways to order $X, Y, Z$. Therefore, with $u=x \leq y \leq z = v$,
\begin{align*}
  f_{L,M}(u,v) \d u \d v= 6 f_{X}(x) (F_{Y}(z)-F_{Y}(x)) f_{Z}(z) \d x \d z
\end{align*}
Since $\partial(u,v)/\partial (x, z) = 1$, 
\begin{align*}
  f_{L,M}(u,v) = 6 f_{X}(u) (F_{Y}(v)-F_{Y}(u)) f_{Z}(v)\1{u\leq v}.
\end{align*}

Here is a neat method with CDFs.
\begin{align*}
  F_{L,M}(u,v) 
&= \P{L\leq u, M\leq v}\\
&= \P{M\leq v} - \P{L\geq u, M\leq v}\\
&= (F_{X}(v))^{3} - (F_{X}(v) - F_{X}(u))^{3}.
\end{align*}
Taking $\partial_{v}\partial_{u}$ gives the same result as above.
\end{solution}
\end{exercise}



\opt{all-solutions-at-end}{
\clearpage
\Closesolutionfile{hint}
\clearpage
\Closesolutionfile{ans}
\section{Hints}
\input{hint}
\section{Solutions}
\input{ans}
}



\end{document}
